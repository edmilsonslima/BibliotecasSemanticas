# -*- coding: utf-8 -*-
"""webcrawlervAuletefinalJSON.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kOC_iS9JNTTMClqXz78ptHAZqINVoGrg
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import json  # Para salvar os resultados em JSON

# Configuração do site
BASE_URL = "https://www.aulete.com.br"
HEADERS = {
    "User-Agent": "Chrome/114.0.0.0"
}

# Função para obter o conteúdo HTML da página
def get_soup(url):
    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code == 200:
            return BeautifulSoup(response.text, "html.parser")
        else:
            print(f"Erro ao acessar {url}: {response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
        print(f"Erro de conexão: {e}")
        return None

# Função para extrair a microestrutura de uma palavra
def extrair_microestrutura(url):
    soup = get_soup(url)
    if not soup:
        return None

    print(f"\nExtraindo microestrutura de: {url}\n")

    try:
        # Palavra
        palavra = soup.find("h2").text.strip()  # Nome da palavra

        # Ortografia
        ortografia = soup.find("span", class_="silabas separacao-silaba").text.strip()  # Nome da ortografia
        ortografia_texto = ortografia.strip() if ortografia else "ortografia não encontrada"

        # Fonética
        fonetica = soup.find("span", class_="ort")
        fonetica_texto = fonetica.text.strip() if fonetica else "não encontrada"

        # Gramática
        gramatica = soup.find("p", class_="classificacao1")
        gramatica_texto = gramatica.text.strip() if gramatica else "não encontrada"

        # Definições
        definicoes = soup.find_all("p")
        lista_definicoes = [def_.text.strip() for def_ in definicoes if def_]  # Extrai as definições
        if lista_definicoes:  # Remove a primeira linha de definições, se existir
            lista_definicoes = lista_definicoes[1:]

        # Filtra as definições para evitar repetição com ortografia, fonética ou gramática
        lista_definicoes = [
            definicao
            for definicao in lista_definicoes
            if definicao not in {ortografia_texto, fonetica_texto, gramatica_texto}
        ]

        return {
            "palavra": palavra,
            "ortografia": ortografia_texto,
            "fonetica": fonetica_texto,
            "gramatica": gramatica_texto,
            "definicoes": lista_definicoes,
        }
    except Exception as e:
        print(f"Erro ao extrair microestrutura: {e}")
        return None

# Função principal do crawler para várias palavras
def crawler_aulete(lista_palavras):
    resultados = {}
    for palavra in lista_palavras:
        url_palavra = f"{BASE_URL}/{palavra}"
        print(f"Processando palavra: {palavra} - URL: {url_palavra}")
        microestrutura = extrair_microestrutura(url_palavra)
        if microestrutura:
            resultados[palavra] = microestrutura
        else:
            print(f"Falha ao processar a palavra: {palavra}")
        time.sleep(1)  # Respeitar o servidor com um intervalo entre as requisições
    return resultados

# Lista de palavras para busca
palavras = ["pessoa"]

# Iniciar o crawler
resultado_microestrutura = crawler_aulete(palavras)

# Salvar os resultados em um arquivo JSON
with open("resultado.json", "w", encoding="utf-8") as file:
    json.dump(resultado_microestrutura, file, ensure_ascii=False, indent=4)

# Exibir os resultados no terminal
for palavra, dados in resultado_microestrutura.items():
    print(f"\nPalavra: {dados['palavra']}")
    print(f"Ortografia: {dados['ortografia']}")
    print(f"Fonética: {dados['fonetica']}")
    print(f"Gramática: {dados['gramatica']}")
    print("Definições:")
    for definicao in dados['definicoes']:
        print(f"- {definicao}")